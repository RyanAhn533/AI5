import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPool2D, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split
import time
import datetime

# 데이터 경로 설정
np_path = 'c:/프로그램/ai5/_data/kaggle/Biggest_gender/'

# 데이터 로드
x_train1 = np.load(np_path + 'bigwoman_x_train1.npy')
x_train2 = np.load(np_path + 'man_x_train1.npy')
y_train1 = np.load(np_path + 'bigwoman_y_train1.npy')
y_train2 = np.load(np_path + 'man_y_train1.npy')

print(x_train1.shape)
print(x_train2.shape)
print(y_train1.shape)
print(y_train2.shape)

# 데이터 병합
x = np.concatenate((x_train1, x_train2))
y = np.concatenate((y_train1, y_train2))

# 데이터 전처리 및 이미지 데이터 제너레이터 설정
train_datagen = ImageDataGenerator(
    rescale=1./255,
    horizontal_flip=True, 
    vertical_flip=True, 
    width_shift_range=0.1, 
    height_shift_range=0.1, 
    rotation_range=1, 
    zoom_range=0.2, 
    shear_range=0.7, 
    fill_mode="nearest"
)

test_datagen = ImageDataGenerator(rescale=1./255)

path1 = "C:\\프로그램\\ai5\\_data\\image\\me\\"
xy_test = test_datagen.flow_from_directory(
    path1, target_size=(100, 100),
    batch_size=30000, 
    class_mode='binary',
    color_mode='rgb',
    shuffle=True
)

# 학습 및 테스트 데이터 분할
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.9, random_state=5656)

# 모델 정의
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(100, 100, 3), padding='same'),
    MaxPool2D(),
    Dropout(0.25),
    BatchNormalization(),
    Conv2D(64, (3,3), activation='relu', padding='same'),
    MaxPool2D(),
    Dropout(0.25),
    BatchNormalization(),
    Conv2D(128, (3,3), activation='relu', padding='same'),
    MaxPool2D(),
    Dropout(0.25),
    BatchNormalization(),
    Conv2D(64, (3,3), activation='relu', padding='same'),
    MaxPool2D(),
    Dropout(0.25),
    Flatten(),
    Dense(1024, activation='relu'),
    Dense(512, activation='relu'),
    Dense(1, activation='sigmoid')
])

# 모델 컴파일
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'mse'])

# 콜백 함수 설정
date = datetime.datetime.now().strftime

("%m%d_%H%M")
path = 'C:\\ai5\\_save\\keras45\\k45_07\\'
filename = '{epoch:04d}-{val_loss:.4f}.hdf5'
filepath = "".join([path, 'k45_07_', date, '_', filename])

es = EarlyStopping(monitor='val_loss', mode='min', patience=30, verbose=1, restore_best_weights=True)
mcp = ModelCheckpoint(monitor='val_loss', mode='auto', verbose=1, save_best_only=True, filepath=filepath)

# 모델 학습
start_time = time.time()
model.fit(train_datagen.flow(x_train, y_train, batch_size=32), epochs=30, validation_data=(x_test, y_test), callbacks=[es, mcp])
end_time = time.time()

# 모델 평가
loss, acc, mse = model.evaluate(x_test, y_test, verbose=1)
print(f'loss: {loss}, acc: {round(acc, 5)}, mse: {mse}')
print("걸린 시간 :", round(end_time - start_time, 2), '초')

# 예측
y_pre = np.round(model.predict(xy_test, batch_size=1))
